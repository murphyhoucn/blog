

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505301726222.png">
  <link rel="icon" href="https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505301728148.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Murphy">
  <meta name="keywords" content="Murphy,CosmicDusty,murphy,cosmicdusty,cn">
  
    <meta name="description" content="The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale.">
<meta property="og:type" content="article">
<meta property="og:title" content="ILSVRCè®ºæ–‡">
<meta property="og:url" content="https://blog.cosmicdusty.cc/post/CV/ILSVRC/index.html">
<meta property="og:site_name" content="CosmicDusty - Blog">
<meta property="og:description" content="The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/imagenet_banner.jpeg">
<meta property="article:published_time" content="2023-05-04T09:41:24.000Z">
<meta property="article:modified_time" content="2025-05-31T05:05:57.788Z">
<meta property="article:author" content="Murphy">
<meta property="article:tag" content="ILSVRC">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/imagenet_banner.jpeg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>ILSVRCè®ºæ–‡ - CosmicDusty - Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.cosmicdusty.cc","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"16d93d5abbd7ea15ec53daed5459ebf7","google":null,"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"JijHNKemeVNog4wF7g9I5TR3","app_key":"FvsKdxHOP2aG5rHrHby5aV0b","server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null},"gtag":null,"woyaola":null,"cnzz":null},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?16d93d5abbd7ea15ec53daed5459ebf7";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>CosmicDusty</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>é¦–é¡µ</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>å½’æ¡£</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>åˆ†ç±»</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>æ ‡ç­¾</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>å…³äº</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>å‹é“¾</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/imagenet_banner.jpeg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="ILSVRCè®ºæ–‡"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Murphy
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-05-04 17:41" pubdate>
          2023å¹´5æœˆ4æ—¥ ä¸‹åˆ
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          1.4k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          12 åˆ†é’Ÿ
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">ILSVRCè®ºæ–‡</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="ILSVRC"><a href="#ILSVRC" class="headerlink" title="ILSVRC"></a>ILSVRC</h1><h2 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h2><iframe width="560" height="315" src="https://www.youtube.com/embed/40riCqvRoMs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>ä¸€åˆ‡çš„èµ·æºï¼šCVPR 2009ï¼Œ*<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/5206848">ImageNet: A large-scale hierarchical image database</a>*</p>
<blockquote>
<p><strong>ImageNet</strong> is an image database organized according to the <a target="_blank" rel="noopener" href="http://wordnet.princeton.edu/">WordNet</a> hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been <a target="_blank" rel="noopener" href="https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/">instrumental</a> in advancing computer vision and deep learning research. The data is available for free to researchers for non-commercial use.<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="[ImageNet (image-net.org)](https://www.image-net.org/)">[2]</span></a></sup></p>
</blockquote>
<p>å¦å¤–è¿˜æœ‰ä¸€ç¯‡2015å¹´çš„è®ºæ–‡ï¼Œ*<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0575">ImageNet Large Scale Visual Recognition Challenge</a>*</p>
<ul>
<li>14,192,122 million images, 21841 thousand categories</li>
</ul>
<p>ImageNetæ•°æ®é›†çš„å‡ºç°ï¼Œç›´æ¥è¯æ˜äº†åºå¤§çš„æ•°æ®æ˜¯å¯ä»¥æ¨åŠ¨è®¡ç®—æœºè§†è§‰çš„è¿›æ­¥çš„ï¼</p>
<h2 id="ILSVRC-1"><a href="#ILSVRC-1" class="headerlink" title="ILSVRC"></a>ILSVRC</h2><p>ILSVRCæ— è®ºæ€ä¹ˆæ¥ä»‹ç»ï¼Œéƒ½ä¸å¦‚<a target="_blank" rel="noopener" href="https://image-net.org/challenges/LSVRC/">ImageNet</a>å®˜æ–¹ç»™å‡ºçš„ä»‹ç»æ¥å¾—ç›´æ¥æ˜äº†ï¼</p>
<blockquote>
<p>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale.<br>One high level motivation is to allow researchers to compare progress in detection across a wider variety of objects â€“ taking advantage of the quite expensive labeling effort.<br>Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[ImageNet (image-net.org)](https://www.image-net.org/challenges/LSVRC/)">[1]</span></a></sup></p>
</blockquote>
<p>ILSVRCä»2010å¹´åˆ°2017å¹´ï¼Œæ€»å…±ä¸¾åŠäº†å…«å±Šï¼Œä½†ç›´åˆ°2012å¹´AlexNetçš„æ¨ªç©ºå‡ºä¸–ï¼Œæ‰ä½¿å¾—ILSVRCå¾—åˆ°äº†æ›´å¤šäººçš„å…³æ³¨ã€‚åœ¨2017å¹´ä¹‹åï¼Œæ¨¡å‹åœ¨ImageNetæ•°æ®é›†ä¸Šçš„é”™è¯¯ç‡å·²ç»è¢«â€œåˆ·çˆ†â€äº†ï¼Œå†ç»§ç»­ä¸‹å»ä¹Ÿå·²ç»æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰äº†ï¼ŒImageNetå·²ç»å®Œæˆäº†å®ƒæœ€åˆçš„ä½¿å‘½äº†ï¼</p>
<h1 id="ILSVRC-2010"><a href="#ILSVRC-2010" class="headerlink" title="ILSVRC 2010"></a>ILSVRC 2010</h1><p>2010å¹´åªæœ‰åˆ†ç±»ä»»åŠ¡</p>
<blockquote>
<p><strong>Winner: NEC-UIUC</strong><br>Yuanqing Lin, Fengjun Lv, Shenghuo Zhu, Ming Yang, Timothee Cour, Kai Yu (NEC). LiangLiang Cao, Zhen Li, Min-Hsuan Tsai, Xi Zhou, Thomas Huang (UIUC). Tong Zhang (Rutgers).<br>[<a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/ILSVRC2010_NEC-UIUC.pdf">PDF</a>] NB: <em>This is unpublished work. Please contact the authors if you plan to make use of any of the ideas presented.</em></p>
</blockquote>
<ul>
<li>Fast descriptor coding</li>
<li>Large-scale SVM classification</li>
</ul>
<blockquote>
<p><strong>Honorable mention: XRCE</strong><br>Jorge Sanchez, Florent Perronnin, Thomas Mensink (XRCE)<br>[<a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/ILSVRC2010_XRCE.pdf">PDF</a>] NB: <em>This is unpublished work. Please contact the authors if you plan to make use of any of the ideas presented.</em></p>
</blockquote>
<ul>
<li>Fisher Vector</li>
</ul>
<h1 id="ILSVRC-2011"><a href="#ILSVRC-2011" class="headerlink" title="ILSVRC 2011"></a>ILSVRC 2011</h1><p>Categorization&amp;Localization</p>
<blockquote>
<p>Classification Winners: <a target="_blank" rel="noopener" href="http://www.xrce.xerox.com/Research-Development/Document-Content-Laboratory/Textual-Visual-Pattern-Analysis-TVPA">XRCE</a><br><a target="_blank" rel="noopener" href="http://www.xrce.xerox.com/Research-Development/Document-Content-Laboratory/Textual-Visual-Pattern-Analysis-TVPA/Large-scale-classification-and-retrieval">Florent Perronnin, Jorge Sanchez</a><br>[<a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2011/ilsvrc11.pdf">PDF</a>] Compressed Fisher vectors for Large Scale Visual Recognition</p>
</blockquote>
<blockquote>
<p>Detection Winners: <a target="_blank" rel="noopener" href="http://koen.me/research/selectivesearch/">University of Amsterdam &amp; University of Trento</a><br><a target="_blank" rel="noopener" href="http://koen.me/research/">Koen van de Sande</a>, Jasper Uijlings<br>Arnold Smeulders, Theo Gevers, Nicu Sebe, Cees Snoek<br>[<a target="_blank" rel="noopener" href="http://koen.me/research/pub/ILSVRC2011-UvATrento-SelectiveSearch.pdf">PDF</a>] Segmentation as Selective Search for Object Recognition</p>
</blockquote>
<h1 id="ILSVRC-2012"><a href="#ILSVRC-2012" class="headerlink" title="ILSVRC 2012"></a>ILSVRC 2012</h1><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2012/results.html">results</a></p>
</li>
<li><p>review [ <a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/ilsvrc2012.pdf">slides</a> ]</p>
</li>
<li><p>Task 1: Classification: SuperVision [ <a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/supervision.pdf">slides</a> ];OXFORD_VGG team [ <a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/oxford_vgg.pdf">slides</a> ] </p>
</li>
<li><p>Task 2: Classification with localization: SuperVision</p>
</li>
<li><p>Task 3: Fine-grained classification: ISI [ <a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/isi.pdf">slides</a> ]</p>
</li>
</ul>
<h2 id="AlexNet-NeurIPS-2012-âœ…"><a href="#AlexNet-NeurIPS-2012-âœ…" class="headerlink" title="AlexNet (NeurIPS 2012)âœ…"></a>AlexNet (NeurIPS 2012)âœ…</h2><blockquote>
<p>ğŸ“„paper: <em><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks (neurips.cc)</a></em></p>
<p>ğŸ’»code: </p>
</blockquote>
<ul>
<li><p><strong>Deep</strong> Convolutional Neural Network</p>
</li>
<li><p>No any unsupervised pre-training</p>
<ul>
<li>åœ¨AlexNetä¹‹å‰ï¼Œæœºå™¨å­¦ä¹ ç•Œè¿˜æ˜¯æ›´å…³å¿ƒunsupervised learning</li>
<li>AlexNetä¹‹åï¼Œéƒ½åœ¨åšsupervised learning</li>
<li>BERTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„å…´èµ·åï¼Œå¤§å®¶åˆæŠŠæ³¨æ„åŠ›æ”¾åˆ°äº†unsupervised learningäº†</li>
</ul>
</li>
<li><p>Computational power</p>
</li>
<li><p>next: video?</p>
</li>
<li><p><strong>æ·±åº¦ç¥ç»ç½‘ç»œå¯¹ä¸€å¼ å›¾ç‰‡è¿›è¡Œè®­ç»ƒåå¾—åˆ°çš„é‚£ä¸ªå‘é‡ï¼Œåœ¨è¯­ä¹‰ç©ºé—´å†…çš„è¡¨ç¤ºç‰¹åˆ«å¥½ï¼è¿™æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸€å¤§å¼ºé¡¹ï¼</strong></p>
</li>
<li><p>larger datasets, more powerful models, less overfitting</p>
</li>
<li><p>a model with large learning capacity</p>
</li>
<li><p>GPU</p>
</li>
<li><p>contributions: largest CNN; GPU; solve overfitting; depth!</p>
</li>
<li><p>two GTX 580 3GB GPUs(å¯¹äºè¿™ç¯‡æ–‡ç« æ¥è¯´ï¼Œä½œè€…å°†æ¨¡å‹åˆ’åˆ†åˆ°äº†ä¸¤ä¸ªGPUä¸Šï¼Œè™½ç„¶å·¥ç¨‹ä¸Šçš„å·¥ä½œé‡å¾ˆå¤§ï¼Œä½†å¤§å®¶éƒ½ä¸æ€ä¹ˆcare)</p>
</li>
<li><p>ImageNetä¸­çš„å›¾åƒå¤§å°éƒ½ä¸ä¸€æ ·ï¼Œä½œè€…å°†å…¶é¢„å¤„ç†ä¸º256Ã—256</p>
<ul>
<li>AlexNetæ²¡ç”¨ä½¿ç”¨SIFTè¿™ç§ç‰¹å¾æå–æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥å¯¹raw RGBå›¾åƒè¿›è¡Œå¤„ç†ï¼Œè¿™ç§ç«¯åˆ°ç«¯ï¼ˆEnd to Endï¼‰çš„å·¥ä½œå¯¹åç»­çš„å·¥ä½œæœ‰é‡å¤§å½±å“ã€‚</li>
</ul>
</li>
<li><p>detail</p>
<ul>
<li>SGD; batch size:128; momentum: 0.9; weight decay: 0.0005</li>
<li>initialized the weight: 0-means Gaussian distribution with standard deviation 0.01</li>
<li>initialized the biases</li>
<li>equal learning rate for all layers; initialized the learning rate 0.01</li>
</ul>
</li>
<li><p>ä¸Šå±‚ç¥ç»å…ƒå­¦åˆ°çš„æ˜¯â€œå…¨å±€â€ç‰¹å¾ï¼Œå¦‚å½¢çŠ¶ï¼›åº•å±‚ç¥ç»å…ƒå­¦åˆ°çš„æ˜¯â€œå±€éƒ¨â€ç‰¹å¾ï¼Œå¦‚çº¹ç†ã€‚</p>
</li>
</ul>
<p>ğŸš©<strong>Contribution 1: Architecture</strong></p>
<ul>
<li><p>ReLU</p>
<ul>
<li>Saturating nonlinearities: tanh or sigmoid</li>
<li>Non-saturating nonlinearities: ReLU, $ f(x) &#x3D; max(0, x)$</li>
<li><em><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf">Rectified Linear Units Improve Restricted Boltzmann Machines (toronto.edu)</a></em></li>
</ul>
</li>
<li><p>Multi GPUs</p>
<ul>
<li>å¤ªå·¥ç¨‹äº†ï¼</li>
</ul>
</li>
<li><p>Local Response Normalization</p>
<ul>
<li>ç°åœ¨å·²ç»æœ‰æ›´å¥½çš„NormalizationæŠ€æœ¯äº†</li>
</ul>
</li>
<li><p>Overlapping Pooling</p>
</li>
</ul>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230504230831028.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="ILSVRC2012çš„æŠ¥å‘Š"><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="[supervision.pdf (image-net.org)](https://www.image-net.org/static_files/files/supervision.pdf)">[3]</span></a></sup></p>
<ul>
<li>è¿™ä¹ˆé•¿çš„ç½‘ç»œç»“æ„å°±æ˜¯å°†ä¸€å¼ å›¾åƒçš„åƒç´ ä¿¡æ¯è½¬åŒ–æˆäº†æœ€ç»ˆé•¿ä¸º1000çš„å‘é‡ã€‚è¿™ä¸ªå‘é‡äº†ä¸°å¯ŒåŒ…å«çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆæœºå™¨çœ‹å¾—æ‡‚ï¼‰ï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„ä¸€å¤§ä¼˜åŠ¿æ‰€åœ¨ï¼</li>
<li>æ‰€ä»¥è¯´æœºå™¨å­¦ä¹ å°±æ˜¯å°†çœ‹å¾—æ‡‚çš„ä¸œè¥¿ä½¿ç”¨æ¨¡å‹è¿›è¡Œå‹ç¼©ï¼Œå‹ç¼©ä¸ºæœºå™¨çœ‹åˆ°æ‡‚å¾—ä¸œè¥¿ï¼Œè¿›è€Œè¿›è¡Œåç»­æ“ä½œã€‚</li>
<li>åé¢ä¸¤ä¸ª4096çš„å…¨è¿æ¥å‚æ•°é‡å¤ªå¤§äº†ï¼</li>
</ul>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230504230927317.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="è®ºæ–‡ä¸­çš„æ¨¡å‹"></p>
<ul>
<li>ä¸ºäº†åœ¨ä¸¤ä¸ªGPUä¸Šè·‘è¿™ä¸ªæ¨¡å‹ï¼ŒAlexç”¨äº†å¤§é‡çš„ä»£ç æŠŠæ¨¡å‹åˆ‡åˆ†ä¸ºä¸¤åŠè¾¹ï¼Œåˆ†åˆ«åœ¨å•ä¸ªGPUè¿›è¡Œè®­ç»ƒï¼Œæå¾—è¿™ä¸ªç¤ºæ„å›¾éå¸¸å¤æ‚ï¼</li>
<li>AlexNetç”¨çš„è¿™ä¸ªâ€œæ¨¡å‹å¹¶è¡Œâ€åœ¨æ­¤ä¹‹åçš„å¾ˆé•¿æ—¶é—´éƒ½ç”¨çš„ä¸å¤šï¼ä½†æ˜¯<em>ï¼ˆæ²¡é”™ï¼Œç§‘æŠ€æ˜¯ä¸ªç¯ï¼Œåˆå›æ¥äº†ï¼‰</em>ï¼Œéšç€GPTå’ŒBERTçš„å‡ºç°ï¼Œâ€œæ¨¡å‹å¹¶è¡Œâ€å†æ¬¡è¢«æèµ·ï¼ç‰¹åˆ«æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ã€‚</li>
</ul>
<p>ğŸš©<strong>Contribution 2: reduce overfitting</strong></p>
<ul>
<li>Data Augmentation<ul>
<li>å°†256Ã—256çš„å›¾åƒéšæœºæŠ å‡ºå‡ ä¸ª224Ã—224çš„åŒºåŸŸï¼Œæ‰©å¤§äº†æ•°æ®é‡</li>
<li>PCAï¼Œä¸»æˆåˆ†åˆ†æï¼Œåœ¨é¢œè‰²é€šé“ä¸Šåšä¸€äº›å˜æ¢</li>
</ul>
</li>
<li>Dropout<ul>
<li><em><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~nitish/dropout/">www.cs.toronto.edu/~nitish&#x2F;dropout&#x2F;</a></em></li>
<li>æ–‡ç« å¯¹Dropoutçš„ç†è§£å’Œç°åœ¨å¤§å®¶å¯¹å…¶çš„ç†è§£å·²ç»ä¸ä¸€æ ·äº†ï¼Œç°åœ¨å¤§å®¶è®¤ä¸ºDropoutæ˜¯ä¸€ç§<strong>æ­£åˆ™é¡¹</strong></li>
</ul>
</li>
</ul>
<h1 id="ILSVRC-2013"><a href="#ILSVRC-2013" class="headerlink" title="ILSVRC 2013"></a>ILSVRC 2013</h1><h2 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h2><blockquote>
<p>ğŸ“„paper: <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53">Visualizing and Understanding Convolutional Networks | SpringerLink</a></p>
<p>ğŸ’»code:</p>
</blockquote>
<p>Matthew D Zeiler, New York University<br>Rob Fergus, New York University</p>
<p>The approach is based on a combination of large convolutional networks with a range of different architectures. The choice of architectures was assisted by visualization of model featuresâ€™ using a deconvolutional network, as described in Zeiler et. al â€œAdaptive Deconvolutional Networks for Mid and High Level Feature Learningâ€, ICCV 2011.  </p>
<p>Each model is trained on a single Nvidia GPU for more than one week. Data is augmented by resizing the images to 256x256 pixels and then selecting random 224x224 pixel crops and horizontal flips from each example. This data augmentation is combined with the Dropout method of Hinton et al. (â€œImproving neural networks by preventing co-adaptation of feature detectorsâ€), which prevents overfitting in these large networks.</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1311.2901">[1311.2901] Visualizing and Understanding Convolutional Networks (arxiv.org)</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/shine-lee/p/11563237.html">ZFNet(2013)åŠå¯è§†åŒ–çš„å¼€ç«¯ - shine-lee - åšå®¢å›­ (cnblogs.com)</a></p>
<h2 id="Clarifai"><a href="#Clarifai" class="headerlink" title="Clarifai"></a>Clarifai</h2><p>é©¬ä¿®Â·æ³½å‹’ï¼ˆMatthew Zeilerï¼‰åˆ©ç”¨2013å¹´èµ¢å¾—ImageNetæŒ‘æˆ˜èµ›æ—¶çš„ç¨‹åºåˆ›åŠäº†Clarifaiå…¬å¸ï¼Œç›®å‰è·å¾—äº†4000ä¸‡ç¾å…ƒé£é™©æŠ•èµ„ã€‚<br><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/814047">æœºå™¨ä¹‹å¿ƒç‹¬å®¶å¯¹è¯Clarifaiåˆ›å§‹äººï¼šä»å›¾åƒè¯†åˆ«åˆ°æ— é™å¯èƒ½-é˜¿é‡Œäº‘å¼€å‘è€…ç¤¾åŒº (aliyun.com)</a></p>
<p>Clarifai</p>
<p>Matthew Zeiler, Clarifai</p>
<p>A large deep convolutional network is trained on the original data to classify each of the 1,000 classes. The only preprocessing done to the data is subtracting a per-pixel mean. To augment the amount of training data, the image is downsampled to 256 pixels and a random 224 pixel crop is taken out of the image and randomly flipped horizontally to provide more views of each example. Additionally, the dropout technique of Hinton et al. â€œImproving neural networks by preventing co-adaptation of feature detectorsâ€ was utilized to further prevent overfitting.  </p>
<p>The architecture contains 65M parameters trained for 10 days on a single Nvidia GPU. By using a novel visualization technique based on the deconvolutional networks of Zeiler et. al, â€œAdaptive Deconvolutional Networks for Mid and High Level Feature Learningâ€, it became clearer what makes the model perform, and from this a powerful architecture was chosen. Multiple such models were averaged together to further boost performance.</p>
<h2 id="OverFeat"><a href="#OverFeat" class="headerlink" title="OverFeat"></a>OverFeat</h2><p>OverFeat - NYU</p>
<p>Pierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun</p>
<p>Our submission is based on an integrated framework for using Convolutional Networks for <strong>classiï¬cation, localization and detection</strong>. We use a multiscale and sliding window approach, efficiently implemented within a ConvNet. This not only improves classiï¬cation performance, but naturally allows the prediction of one or more objectsâ€™ bounding boxes within the image. The same basic framework was applied to all three tasks. For the classification task, we vote among different views presented to the network. For localization and detection, each sliding window classification is refined using a regressor trained to predict bounding boxes; we produce final predictions by combining the regressor outputs.</p>
<h1 id="ILSVRC-2014"><a href="#ILSVRC-2014" class="headerlink" title="ILSVRC 2014"></a>ILSVRC 2014</h1><h2 id="GoogLeNet-CVPR-2015-âœ…"><a href="#GoogLeNet-CVPR-2015-âœ…" class="headerlink" title="GoogLeNet (CVPR 2015)âœ…"></a>GoogLeNet (CVPR 2015)âœ…</h2><blockquote>
<p>ğŸ“„paper: </p>
<ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.4842">1409.4842] Going Deeper with Convolutions (arxiv.org)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html">CVPR 2015 Open Access Repository (cv-foundation.org)</a></li>
</ul>
<p>ğŸ’»code: </p>
</blockquote>
<ul>
<li>a deep convolutional neural network architecture codenamed Inception</li>
<li>the main hallmark of this architecture is the improved utilization of the <strong>computing resources</strong> inside the network</li>
<li>increasing the depth and width of the network while keeping the computational budget constant.</li>
</ul>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230517202517369.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="inception model"></p>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230517202539308.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="GoogLeNet"></p>
<h2 id="VGG-ICLR-2015-âœ…"><a href="#VGG-ICLR-2015-âœ…" class="headerlink" title="VGG (ICLR 2015)âœ…"></a>VGG (ICLR 2015)âœ…</h2><blockquote>
<p>ğŸ“„paper: [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1556">1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition (arxiv.org)</a></p>
<p>ğŸ’»code: </p>
</blockquote>
<ul>
<li>AlexNet -deeper-&gt; VGG</li>
<li>AlexNetä¸­å‡ºç°çš„å·ç§¯æ ¸å¤§å°ä¸º11Ã—11ï¼Œ7Ã—7å’Œ5Ã—5ï¼Œä½†åœ¨VGGä¸­ï¼Œè¿™ä¹ˆå¤§çš„å·ç§¯æ ¸å·²ç»è§ä¸åˆ°äº†ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯3Ã—3çš„å·ç§¯æ ¸</li>
<li>VGGä¸­ï¼Œä½¿ç”¨äº†3ä¸ª3x3å·ç§¯æ ¸æ¥ä»£æ›¿7x7å·ç§¯æ ¸ï¼Œä½¿ç”¨äº†2ä¸ª3x3å·ç§¯æ ¸æ¥ä»£æ›¿5*5å·ç§¯æ ¸ï¼Œè¿™æ ·åšçš„ä¸»è¦ç›®çš„æ˜¯åœ¨ä¿è¯å…·æœ‰ç›¸åŒæ„ŸçŸ¥é‡çš„æ¡ä»¶ä¸‹ï¼Œæå‡äº†ç½‘ç»œçš„æ·±åº¦ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šæå‡äº†ç¥ç»ç½‘ç»œçš„æ•ˆæœã€‚</li>
</ul>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230517202351878.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="ConvNet configurations"></p>
<h1 id="ILSVRC-2015"><a href="#ILSVRC-2015" class="headerlink" title="ILSVRC 2015"></a>ILSVRC 2015</h1><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2015/results.php">ILSVRC2015 Results (image-net.org)</a></p>
</li>
<li><p>Two main competitions</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2015/index.php#det">Object detection</a> for 200 fully labeled categories.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2015/index.php#loc">Object localization</a> for 1000 categories.</p>
</li>
</ul>
</li>
<li><p>Two taster competitions</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2015/index.php#vid">Object detection from video</a> for 30 fully labeled categories.</li>
<li><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/2015/index.php#scene">Scene classification</a> for 401 categories. Joint with MIT Places team.</li>
</ul>
</li>
</ul>
<h2 id="ResNet-CVPR-2016-âœ…"><a href="#ResNet-CVPR-2016-âœ…" class="headerlink" title="ResNet (CVPR 2016)âœ…"></a>ResNet (CVPR 2016)âœ…</h2><blockquote>
<p>ğŸ“„paper:</p>
<ul>
<li><em>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">1512.03385] Deep Residual Learning for Image Recognition (arxiv.org)</a></em></li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">CVPR 2016 Open Access Repository (thecvf.com)</a></li>
</ul>
<p>ğŸ’»code: </p>
<p>ğŸ”—reference:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349717627">ResNetï¼šæ®‹å·®ç¥ç»ç½‘ç»œ - çŸ¥ä¹ (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/sazass/article/details/116864275">ã€pytorchç³»åˆ—ã€‘ResNetä¸­çš„BasicBlockä¸bottleneck-CSDNåšå®¢</a></li>
<li><a target="_blank" rel="noopener" href="http://zh.d2l.ai/chapter_convolutional-modern/resnet.html">7.6. æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰ â€” åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  2.0.0 documentation (d2l.ai)</a></li>
</ul>
</blockquote>
<iframe width="560" height="315" src="https://www.youtube.com/embed/1PGLj-uKT1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>ğŸš©<strong>intro</strong></p>
<ul>
<li>deeper neural network are more difficult to train!</li>
<li>residual learning framework: more layers but lower complexity</li>
<li>residual: $F(x)&#x3D;H(x)-x$<ul>
<li>æ®‹å·®è¿æ¥å¯ä»¥è¿™æ ·ç†è§£ï¼Œå¦‚æœæ–°åŠ äº†è¿™äº›å±‚å¹¶ä¸èƒ½ä½¿æ¨¡å‹è¡¨ç°æ›´å¥½çš„è¯ï¼Œå°±è®©è¿™ä¸ª$H(x)-&gt;0$.</li>
<li>æ®‹å·®è¿æ¥å°†æ¨¡å‹çš„å¤æ‚åº¦é™ä½äº†</li>
</ul>
</li>
<li>shortcut connections(è¿™ä¹Ÿæ˜¯å¾ˆä¹…ä¹‹å‰æå‡ºçš„ä¸œè¥¿)<ul>
<li>the shortcut connections simply perform <strong>identity mapping</strong></li>
<li>identity shortcut connections add neither extra parameters nor computational complexity</li>
</ul>
</li>
</ul>
<p>ğŸš©<strong>related work</strong></p>
<ul>
<li>Resudial Representations</li>
<li>Shortcut Connections</li>
</ul>
<p>ğŸš©<strong>exp</strong></p>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230505112627441.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="Architectures for ImageNet"></p>
<p><img src="https://murhyimgur.oss-cn-beijing.aliyuncs.com/img/image-20230505113324320.png" srcset="https://murhyimgur.oss-cn-beijing.aliyuncs.com/website/loading.gif" lazyload alt="basic block and bottleneck block"></p>
<h1 id="ILSVRC-2016"><a href="#ILSVRC-2016" class="headerlink" title="ILSVRC 2016"></a>ILSVRC 2016</h1><h2 id="Trimps-Soushen"><a href="#Trimps-Soushen" class="headerlink" title="Trimps-Soushen"></a>Trimps-Soushen</h2><blockquote>
<p>ğŸ“„paper:</p>
<p>ğŸ’»code: </p>
</blockquote>
<h2 id="ResNeXt-CVPR-2017"><a href="#ResNeXt-CVPR-2017" class="headerlink" title="ResNeXt(CVPR 2017)"></a>ResNeXt(CVPR 2017)</h2><blockquote>
<p>ğŸ“„paper: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html">CVPR 2017 Open Access Repository (thecvf.com)</a></p>
<p>ğŸ’»code: </p>
</blockquote>
<h1 id="ILSVRC-2017"><a href="#ILSVRC-2017" class="headerlink" title="ILSVRC 2017"></a>ILSVRC 2017</h1><h2 id="SENet-CVPR-2018"><a href="#SENet-CVPR-2018" class="headerlink" title="SENet(CVPR 2018)"></a>SENet(CVPR 2018)</h2><blockquote>
<p>ğŸ“„paper: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html">CVPR 2018 Open Access Repository (thecvf.com)</a></p>
<p>ğŸ’»code:</p>
</blockquote>
<h1 id="2017"><a href="#2017" class="headerlink" title="2017-"></a>2017-</h1><p>kaggle</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hzhj2007/article/details/87388240">é‚£äº›å¹´æˆ‘ä»¬ä¸€èµ·è¿½è¿‡çš„ILSVRCå† å†›_hzhjçš„åšå®¢-CSDNåšå®¢</a></p>
<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.image-net.org/challenges/LSVRC/">ImageNet (image-net.org)</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> â†©</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.image-net.org/">ImageNet (image-net.org)</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref"> â†©</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.image-net.org/static_files/files/supervision.pdf">supervision.pdf (image-net.org)</a>
<a href="#fnref:3" rev="footnote" class="footnote-backref"> â†©</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="#fnref:4" rev="footnote" class="footnote-backref"> â†©</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://github.com/weiaicunzai/pytorch-cifar100">weiaicunzai&#x2F;pytorch-cifar100: Practice on cifar100(ResNet, DenseNet, VGG, GoogleNet, InceptionV3, InceptionV4, Inception-ResNetv2, Xception, Resnet In Resnet, ResNext,ShuffleNet, ShuffleNetv2, MobileNet, MobileNetv2, SqueezeNet, NasNet, Residual Attention Network, SENet, WideResNet) (github.com)</a>[^xx]:<a target="_blank" rel="noopener" href="https://www.kaggle.com/getting-started/149448">ImageNet Winning CNN Architectures (ILSVRC) | Data Science and Machine Learning | Kaggle</a>
<a href="#fnref:5" rev="footnote" class="footnote-backref"> â†©</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/CV/" class="category-chain-item">CV</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/ILSVRC/" class="print-no-link">#ILSVRC</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>ILSVRCè®ºæ–‡</div>
      <div>https://blog.cosmicdusty.cc/post/CV/ILSVRC/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Murphy</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2023å¹´5æœˆ4æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/post/Knowledge/DockerBeginner/" title="Dockerå…¥é—¨">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Dockerå…¥é—¨</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/post/Tools/MagicalWebPages/" title="ä¸å¯é”™è¿‡çš„ç½‘é¡µ">
                        <span class="hidden-mobile">ä¸å¯é”™è¿‡çš„ç½‘é¡µ</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"JijHNKemeVNog4wF7g9I5TR3-gzGzoHsz","appKey":"FvsKdxHOP2aG5rHrHby5aV0b","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>ç›®å½•</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a rel="nofollow noopener"><span>Powered by</span></a> <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span><b>Hexo</b></span></a> <i class="iconfont icon-love"></i> <a rel="nofollow noopener"><span>Themed by</span></a> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span><b>Fluid</b></span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡
        <span id="busuanzi_value_site_pv"></span>
        æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•°
        <span id="busuanzi_value_site_uv"></span>
        äºº
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
</body>
</html>
